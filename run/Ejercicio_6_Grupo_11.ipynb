{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b3f26b",
   "metadata": {},
   "source": [
    "## 6. GDA\n",
    "Implementen su propia versión de un modelo GDA sobre el mismo dataset que el punto anterior y compare sus resultados. ¿Que modelo dió mejor accuracy? ¿Que similitudes o diferencias encuentran en los parámetros obtenidos? ¿Es posible sacar alguna conclusión de estos resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e1dc0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, det\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c1e76f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDA():\n",
    "    def __init__(self):\n",
    "        # Inicializamos los parámetros\n",
    "        self.K = None\n",
    "        self.mu = {}\n",
    "        self.sigma = {}\n",
    "        self.priors = {}\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        ''' Método para entrenar el modelo GDA\n",
    "        X_train: matriz de datos de entrenamiento\n",
    "        y_train: vector de targets de entrenamiento\n",
    "        '''\n",
    "\n",
    "        # Aseguramos que los datos sean del tipo numpy array\n",
    "        X, y = np.array(X_train), np.array(y_train)\n",
    "\n",
    "        # Definimos a K como el vector de clases únicas\n",
    "        self.K = np.unique(y)\n",
    "\n",
    "        # Definimos los parámetros del modelo\n",
    "        # mu: media de cada clase\n",
    "        self.mu = {k: np.mean(X[y == k], axis=0) for k in self.K}\n",
    "\n",
    "        # priors: probabilidad a priori de cada clase\n",
    "        self.priors = {k: np.mean(y == k) for k in self.K}\n",
    "\n",
    "        # sigma: matriz de covarianza de cada clase\n",
    "        # Metemos una regularización, agregando un pequeño valor a la diagonal\n",
    "        # para evitar problemas de singularidad, y asegurar invertibilidad\n",
    "        self.sigma = {k: np.cov(X[y == k], rowvar=False) + 1e-6 * np.eye(X.shape[1]) for k in self.K}\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        ''' Método para predecir la clase de los datos de test\n",
    "        X_test: matriz de datos de testeo\n",
    "        '''\n",
    "\n",
    "        # Aseguramos que los datos sean del tipo numpy array\n",
    "        X = np.array(X_test)\n",
    "        Predict = []\n",
    "        for x in X:\n",
    "            # Calculamos la probabilidad de cada clase\n",
    "            prob_conjunta = np.array([1/np.sqrt(det(self.sigma[k])) * np.exp(-0.5 * np.dot(np.dot((x-self.mu[k]), inv(self.sigma[k])), (x-self.mu[k]).T)) for k in self.K])\n",
    "            probs = {k: prob_conjunta[k] * self.priors[k] for k in self.K}\n",
    "            Predict.append(max(probs, key=probs.get))\n",
    "            \n",
    "        return np.array(Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9be8a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos la clase LDA para comparar con GDA\n",
    "class LDA():\n",
    "    def __init__(self):\n",
    "        # Inicializo los parámetros\n",
    "        self.phi = []\n",
    "        self.mu = []\n",
    "        self.sigma = []\n",
    "        self.K = []\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        ''' Método para entrenar el modelo LDA\n",
    "        X_train: matriz de datos de entrenamiento\n",
    "        y_train: vector de targets de entrenamiento\n",
    "        '''\n",
    "\n",
    "        # Aseguramos que los datos sean del tipo numpy array\n",
    "        X, y = np.array(X_train), np.array(y_train)\n",
    "\n",
    "        # Definimos a K como el vector de clases únicas\n",
    "        self.K = np.unique(y)\n",
    "\n",
    "        # n = filas, m = columnas\n",
    "        n, m = X.shape\n",
    "\n",
    "        # Iteramos sobre las clases\n",
    "        # y calculamos la probabilidad a priori phi y la media mu\n",
    "        for i in self.K:\n",
    "            self.phi.append(len(y[y == i]) / n)\n",
    "            self.mu.append(np.mean(X[y == i], axis=0))\n",
    "        # Aseguramos que phi y mu sean numpy arrays\n",
    "        self.phi = np.array(self.phi)\n",
    "        self.mu = np.array(self.mu)\n",
    "\n",
    "        # Calculamos la matriz de covarianza sigma\n",
    "        self.sigma = np.zeros((m, m))\n",
    "        for i in range(n):\n",
    "            self.sigma += (X[i] - self.mu[y[i]]).reshape(-1, 1).dot((X[i] - self.mu[y[i]]).reshape(1, -1)) / n\n",
    "        # Metemos una regularización, agregando un pequeño valor a la diagonal\n",
    "        # para evitar problemas de singularidad, y asegurar invertibilidad\n",
    "        self.sigma += 1e-6 * np.eye(self.sigma.shape[0])\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        ''' Método para predecir la clase de los datos de test\n",
    "        X_test: matriz de datos de testeo\n",
    "        '''\n",
    "\n",
    "        # Aseguramos que los datos sean del tipo numpy array\n",
    "        X = np.array(X_test)\n",
    "\n",
    "        # Inicializamos el vector de predicciones\n",
    "        Predict = []\n",
    "        for j in range(X.shape[0]):\n",
    "            Probas = []\n",
    "            for i in range(len(self.K)):\n",
    "                # Calculamos la probabilidad a posteriori\n",
    "                Posterior = np.exp(-0.5 * (X[j] - self.mu[i]).dot(inv(self.sigma)).dot((X[j] - self.mu[i]).T))\n",
    "                # La multiplicamos por la probabilidad a priori\n",
    "                Probas.append(self.phi[i] * Posterior)\n",
    "            Probas = np.array(Probas)\n",
    "            # Realizamos la predicción tomando la clase con mayor probabilidad\n",
    "            # y la agregamos al vector de predicciones\n",
    "            Predict.append(self.K[np.argmax(Probas)])\n",
    "            \n",
    "        return np.array(Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "41eddfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDA:0.76\n",
      "LDA:0.89\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargamos el dataset\n",
    "df = pd.read_csv(\"Datos/dataset.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Separar features y labels\n",
    "X = df.drop(columns=[\"label\"])\n",
    "y = df[\"label\"].map({\"'Male'\": 0, \"'Female'\": 1})  # Convertir a valores numéricos\n",
    "\n",
    "# Hago un resample para balancear las clases\n",
    "X_resampled, y_resampled = RandomOverSampler().fit_resample(X, y)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "GDA_Prueba = GDA()\n",
    "GDA_Prueba.fit(X_train,y_train)\n",
    "# Accuracy\n",
    "print('GDA:'+ str(np.mean(GDA_Prueba.predict(X_test) == y_test)))\n",
    "\n",
    "LDA_Prueba = LDA()\n",
    "LDA_Prueba.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "print('LDA:'+ str(np.mean(LDA_Prueba.predict(X_test) == y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248da248",
   "metadata": {},
   "source": [
    "En todas las pruebas, las predicciones que realiza GDA son menos acertadas que las que devuele LDA. Esto puede deberse a que este último asume que las clases comparten la misma matriz de covarianza, lo que resulta en una frontera de decisión lineal. En este dataset, esa suposición parece ajustarse mejor a los datos, permitiendo que LDA generalice mejor y obtenga mayor accuracy. Por otro lado, GDA estima una matriz de covarianza diferente para cada clase, lo que puede llevar a sobreajuste si el número de muestras no es suficientemente grande o si las clases no son realmente tan diferentes en su dispersión. Además, los parámetros obtenidos por LDA suelen ser más estables y menos sensibles al ruido, mientras que GDA puede capturar mejor relaciones no lineales si existen, pero en este caso no aporta ventajas claras. En conclusión, la simplicidad de LDA lo hace más robusto para este problema en particular."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
